## åˆ©ç”¨Milvuså‘é‡æ•°æ®åº“è¿”å›ç›¸ä¼¼æ•°æ®
- [åˆ©ç”¨Milvuså‘é‡æ•°æ®åº“è¿”å›ç›¸ä¼¼æ•°æ®](#åˆ©ç”¨milvuså‘é‡æ•°æ®åº“è¿”å›ç›¸ä¼¼æ•°æ®)
- [æ¨¡å‹é€‰æ‹©ï¼š](#æ¨¡å‹é€‰æ‹©)
- [ç¨‹åºè¿è¡Œæ–¹å¼ï¼š](#ç¨‹åºè¿è¡Œæ–¹å¼)
- [å­˜å‚¨æ—¶ç´¢å¼•æ„é€ è§£é‡Š:](#å­˜å‚¨æ—¶ç´¢å¼•æ„é€ è§£é‡Š)
- [è¯å‘é‡æ„é€ è§£é‡Š:](#è¯å‘é‡æ„é€ è§£é‡Š)
- [albert\_text\_vec.pyä¸­ç±»ä¼¼ç”¨æ³•çš„è§£é‡Š:](#albert_text_vecpyä¸­ç±»ä¼¼ç”¨æ³•çš„è§£é‡Š)

## æ¨¡å‹é€‰æ‹©ï¼š

æ¨¡å‹åç§°: `clue/albert_chinese_tiny`ï¼Œå¯ä»¥é€šè¿‡ä¸‹æ–¹é“¾æ¥ä¸‹è½½:<br>

```txt
https://huggingface.co/clue/albert_chinese_tiny
```

## ç¨‹åºè¿è¡Œæ–¹å¼ï¼š

1. ä¸‹è½½æˆ–å°†`clue/albert_chinese_tiny`æ–‡ä»¶ç§»åŠ¨åˆ°å½“å‰ç›®å½•ï¼›

2. è¿è¡Œ`insert_data_to_milvus.py`æ–‡ä»¶ï¼›

3. è¿è¡Œ`sanic_milvus.py`æ–‡ä»¶ï¼›

4. æ‰“å¼€postmanï¼Œé€‰æ‹©POSTæ¨¡å¼ï¼Œè¾“å…¥ç±»ä¼¼ä»¥ä¸‹çš„URLï¼›

URL: `http://8.140.203.xxx:8848/vector_similarity`<br>

5. é€‰æ‹©`Body`é€‰é¡¹ï¼Œè‡ªå®šä¹‰Valueä¼ å…¥ï¼Œç‚¹å‡»Sendå³å¯ã€‚

Key|Value|Description
---|---|---
usr_input | æ•™å¸ˆ | 

è¿”å›å†…å®¹å¦‚ä¸‹:<br>

> distanceä¸ºç›¸ä¼¼åº¦åº¦é‡; fieldsä¸ºè‡ªå®šä¹‰çš„è¿”å›å­—æ®µ;

```json
{
    "ç›¸ä¼¼åº¦è®¡ç®—çš„ç»“æœä¸ºï¼š": {
        "ç»“æœ1": {
            "id": 161872,
            "distance": 1.0000001192092896,
            "fields": {
                "id": 161872,
                "text": "æ•™å¸ˆ"
            }
        },
        "ç»“æœ2": {
            "id": 161804,
            "distance": 0.9283397197723389,
            "fields": {
                "id": 161804,
                "text": "æ•™å­¦"
            }
        },
        "ç»“æœ3": {
            "id": 28946,
            "distance": 0.9250288009643555,
            "fields": {
                "id": 28946,
                "text": "ä»»è¯¾æ•™å¸ˆ"
            }
        }
    }
}
```

â¤ï¸â¤ï¸â¤ï¸ç»æµ‹è¯•:æˆ‘çš„Milvusä¸­å­˜å‚¨äº†34ä¸‡æ¡å‘é‡æ•°æ®(312ç»´)ï¼Œæ£€ç´¢ä¸€æ¡æ•°æ®çš„è€—æ—¶ä¸º2.7msã€‚<br>

## å­˜å‚¨æ—¶ç´¢å¼•æ„é€ è§£é‡Š:

```python
def create_milvus_collection(collection_name, dim):
    if utility.has_collection(collection_name):
        utility.drop_collection(collection_name)
    
    fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=False),
            FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=500),   
            FieldSchema(name="text_vector", dtype=DataType.FLOAT_VECTOR, dim=dim),
    ]
    schema = CollectionSchema(fields=fields, description='search text')
    collection = Collection(name=collection_name, schema=schema)
    
    index_params = {
        'metric_type': "COSINE",
        'index_type': "HNSW",
        'params': {"nlist": 1024, 'efConstruction': 10, 'M':60}
    }
    collection.create_index(field_name='text_vector', index_params=index_params)
    return collection
```

1. `metric_type`: å®šä¹‰ç”¨äºå‘é‡æœç´¢æ—¶è®¡ç®—å‘é‡ä¹‹é—´ç›¸ä¼¼åº¦çš„åº¦é‡ç±»å‹ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ä½¿ç”¨çš„æ˜¯ "COSINE"ï¼Œè¡¨ç¤ºä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå‘é‡ä¹‹é—´ç›¸ä¼¼åº¦çš„åº¦é‡ã€‚ä½™å¼¦ç›¸ä¼¼åº¦æ˜¯è¡¡é‡ä¸¤ä¸ªå‘é‡æ–¹å‘ä¸Šç›¸ä¼¼ç¨‹åº¦çš„ä¸€ä¸ªåº¦é‡æ ‡å‡†ï¼Œå®ƒçš„å€¼åŸŸä»-1ï¼ˆå®Œå…¨ä¸åŒï¼‰åˆ°1ï¼ˆå®Œå…¨ç›¸åŒï¼‰ï¼Œå…¶ä¸­0é€šå¸¸è¡¨ç¤ºä¸¤ä¸ªå‘é‡æ˜¯æ­£äº¤çš„ï¼Œå³ç‹¬ç«‹æ— å…³ã€‚

2. `index_type`: æŒ‡å®šç”¨äºé›†åˆä¸­çš„å‘é‡å­—æ®µçš„ç´¢å¼•ç±»å‹ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œä½¿ç”¨çš„æ˜¯ "HNSW"ï¼ŒHierarchical Navigable Small Worldï¼ˆåˆ†å±‚å¯å¯¼èˆªå°ä¸–ç•Œå›¾ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å›¾çš„è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ç®—æ³•ï¼Œå®ƒåœ¨å¤§è§„æ¨¡å’Œé«˜ç»´æ•°æ®é›†ä¸Šæä¾›äº†é«˜æ•ˆçš„æœç´¢æ€§èƒ½ã€‚

3. `params`: è¿™æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ç”¨äºæ„å»ºç´¢å¼•æ—¶çš„é¢å¤–å‚æ•°ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæœ‰ä¸‰ä¸ªå‚æ•°ï¼š
   - `nlist`: è¡¨ç¤ºåœ¨ç´¢å¼•ä¸­åˆ’åˆ†çš„èšç±»çš„æ•°é‡ï¼Œè¿™ä¸ªå‚æ•°å¯¹äºç´¢å¼•çš„è´¨é‡å’Œæœç´¢æ€§èƒ½éƒ½æœ‰å½±å“ã€‚
   - `efConstruction`: æ˜¯æ„å»ºæ—¶çš„å¤§å°ï¼Œè¿™ä¸ªå‚æ•°å†³å®šäº†ç´¢å¼•æ„å»ºè¿‡ç¨‹ä¸­çš„ç²¾åº¦å’Œæ€§èƒ½ã€‚é€šå¸¸æ›´é«˜çš„ `efConstruction` å€¼ä¼šæé«˜ç´¢å¼•çš„è´¨é‡ï¼Œä½†åŒæ—¶ä¼šå¢åŠ æ„å»ºç´¢å¼•çš„æ—¶é—´å’Œèµ„æºæ¶ˆè€—ã€‚
   - `M`: ä¸ `HNSW` ç´¢å¼•ç›¸å…³çš„å‚æ•°ï¼Œå®ƒå®šä¹‰äº†å›¾ä¸­èŠ‚ç‚¹çš„æœ€å¤§å‡ºåº¦ï¼Œä¹Ÿå°±æ˜¯æ¯ä¸ªå…ƒç´ å°†è¿æ¥å¤šå°‘ä¸ªé‚»å±…ã€‚`M` è¶Šå¤§ï¼Œæ„å»ºç´¢å¼•çš„æ—¶é—´è¶Šé•¿ï¼Œä½†æœç´¢çš„å‡†ç¡®æ€§å¯èƒ½ä¼šæé«˜ã€‚

æœ€åï¼Œ`create_index` æ–¹æ³•æ˜¯ç”¨æ¥åœ¨é›†åˆçš„ `text_vector` å­—æ®µä¸Šåˆ›å»ºæŒ‡å®šç±»å‹çš„ç´¢å¼•ï¼Œä»¥ä¾¿æ›´é«˜æ•ˆåœ°æ‰§è¡Œæœç´¢æ“ä½œã€‚<br>

## è¯å‘é‡æ„é€ è§£é‡Š:

æˆ‘çš„è¯å‘é‡æ„é€ è¾ƒä¸ºå¤æ‚ï¼Œå…·ä½“ä»£ç å¦‚ä¸‹:<br>

```python
# ä½¿ç”¨tokenizerå°†æ–‡æœ¬ç¼–ç 
encoded_texts = tokenizer(batch_texts, return_tensors='pt', padding=True)
with torch.no_grad():
    # é€šè¿‡æ¨¡å‹è·å–æ–‡æœ¬çš„éšè—çŠ¶æ€
    last_hidden_state = model(**encoded_texts).last_hidden_state
    # è·å–æ³¨æ„åŠ›æ©ç 
    attention_mask = encoded_texts["attention_mask"]
    # å°†éšè—çŠ¶æ€ä¸æ³¨æ„åŠ›æ©ç ç›¸ä¹˜ï¼Œç”¨äºæ¶ˆé™¤paddingçš„å½±å“
    last_hidden_state = last_hidden_state * attention_mask.unsqueeze(-1)
    # å¯¹éšè—çŠ¶æ€æ±‚å’Œï¼Œç”¨äºè·å¾—æ•´ä¸ªæ–‡æœ¬çš„è¡¨ç¤º
    sum_hidden_state = last_hidden_state.sum(dim=1).squeeze()
    # é€šè¿‡æ³¨æ„åŠ›æ©ç çš„å’Œè¿›è¡Œå½’ä¸€åŒ–
    output = sum_hidden_state / attention_mask.sum(dim=1, keepdim=True)
    # è½¬åŒ–ä¸ºnumpyæ•°ç»„
    output = output.numpy()
# å½’ä¸€åŒ–è¾“å‡ºå‘é‡ï¼Œä»¥ä¾¿å‘é‡çš„æ¨¡é•¿ä¸º1
output = output / np.linalg.norm(output, axis=1, keepdims=True).tolist()
```

ä½ å¯èƒ½ä¼šæƒ³ï¼Œæˆ–ç€ä½ å¯èƒ½åœ¨å…¶ä»–åœ°æ–¹è§åˆ°è¿‡ï¼Œç±»ä¼¼ä¸‹é¢è¿™ç§ç®€å•çš„å†™æ³•:<br>

```python
# ä½¿ç”¨tokenizerå°†æ–‡æœ¬ç¼–ç 
encoded_texts = tokenizer(batch_texts, return_tensors='pt', padding=True)
with torch.no_grad():
    output = model(**encoded_texts).last_hidden_state.mean(dim=1).squeeze().numpy()
```

**ğŸš€ğŸš€ğŸš€ä½ ä¹Ÿè®¸ä¼šæƒ³ï¼Œä¸ºä»€ä¹ˆä¸ç”¨è¿™ç§ç®€å•çš„å†™æ³•å‘¢ï¼Ÿ**<br>

æ˜¯çš„ï¼Œä½ å¯ä»¥ä½¿ç”¨è¿™ç§æ–¹æ³•ç›´æ¥è·å–æ¯ä¸ªæ–‡æœ¬çš„å¹³å‡è¯å‘é‡è¡¨ç¤ºã€‚è¿™è¡Œä»£ç å°†å®Œæˆä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š<br>

1. `model(**encoded_texts).last_hidden_state`ï¼šè¿™ä¼šé€šè¿‡æ¨¡å‹è·å¾—æ¯ä¸ªtokençš„è¯å‘é‡è¡¨ç¤ºã€‚`last_hidden_state` æ˜¯æ¨¡å‹æœ€åä¸€å±‚çš„è¾“å‡ºï¼Œå®ƒåŒ…å«äº†æ¯ä¸ªtokençš„éšè—çŠ¶æ€ã€‚

2. `.mean(dim=1)`ï¼šè¿™ä¸ªæ“ä½œä¼šåœ¨ç¬¬ä¸€ä¸ªç»´åº¦ï¼ˆå³sequenceé•¿åº¦çš„ç»´åº¦ï¼‰ä¸Šè®¡ç®—å¹³å‡å€¼ï¼Œå› æ­¤å¯¹äºæ¯ä¸ªæ–‡æœ¬ï¼Œå®ƒä¼šå°†æ‰€æœ‰tokençš„è¯å‘é‡è¿›è¡Œå¹³å‡ï¼Œè¿™æ ·å°±å¾—åˆ°äº†å•ä¸ªå‘é‡è¡¨ç¤ºæ•´ä¸ªæ–‡æœ¬çš„å¹³å‡éšè—çŠ¶æ€ã€‚è¿™å¿½ç•¥äº†ä¸åŒè¯è¯­çš„é‡è¦æ€§å¯èƒ½ä¸åŒï¼Œå¹¶å‡è®¾æ‰€æœ‰è¯è¯­éƒ½åŒç­‰é‡è¦ã€‚

3. `.squeeze()`ï¼šå¦‚æœç»´åº¦ä¸­æœ‰ä»»ä½•å•ç»´åº¦ï¼Œåˆ™æ­¤æ“ä½œä¼šç§»é™¤å®ƒä»¬ã€‚æ¯”å¦‚ï¼Œå¦‚æœç»´åº¦æ˜¯`(batch_size, 1, hidden_size)`ï¼Œå®ƒä¼šå˜æˆ`(batch_size, hidden_size)`ã€‚

4. `.numpy()`ï¼šè¿™å°†æŠŠç»“æœä»PyTorchå¼ é‡è½¬æ¢ä¸ºNumPyæ•°ç»„ï¼Œæ–¹ä¾¿åç»­å¤„ç†æˆ–å­˜å‚¨ã€‚

ğŸš¨ğŸš¨ğŸš¨ä½†æ˜¯ï¼Œä½¿ç”¨`.mean()`å‡½æ•°æœ‰ä¸€ä¸ªç¼ºç‚¹ï¼š**å®ƒä¼šå¯¹æ‰€æœ‰tokensçš„å‘é‡è¿›è¡Œå¹³å‡ï¼ŒåŒ…æ‹¬é‚£äº›å¡«å……çš„tokensï¼ˆå³[PAD] tokensï¼‰ã€‚**è¿™å¯èƒ½ä¼šç¨€é‡Šå®é™…æœ‰æ„ä¹‰è¯æ±‡çš„è¡¨ç¤ºï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†é•¿åº¦ä¸ä¸€çš„å¥å­æ—¶ã€‚å¦‚æœä½ æƒ³è¦ä¸€ä¸ªæ›´åŠ ç²¾ç¡®çš„æ–‡æœ¬è¡¨ç¤ºï¼Œé‚£ä¹ˆåº”è¯¥åªå¯¹å®é™…çš„è¯æ±‡ï¼ˆè€Œéå¡«å……çš„éƒ¨åˆ†ï¼‰è®¡ç®—å¹³å‡å€¼æˆ–è€…å…¶ä»–å½¢å¼çš„èšåˆã€‚<br>

ğŸŒ¿ğŸŒ¿ğŸŒ¿åœ¨ä¹‹å‰çš„ä»£ç ä¸­ï¼Œç¬”è€…é€šè¿‡ä¹˜ä»¥`attention_mask`æ¥ç¡®ä¿åªæœ‰éå¡«å……tokensè¢«è€ƒè™‘åœ¨å†…ï¼Œè¿™æ ·å¯ä»¥è·å¾—æ›´åŠ å‡†ç¡®çš„æ–‡æœ¬è¡¨ç¤ºã€‚<br>

**å¦‚æœä½ å†³å®šä½¿ç”¨`.mean()`ï¼Œè¯·ç¡®ä¿ä½ çš„æ•°æ®é›†ä¸­å¤§éƒ¨åˆ†æ–‡æœ¬çš„é•¿åº¦ç›¸è¿‘ï¼Œè¿™æ ·å¡«å……å¯¹äºç»“æœçš„å½±å“æ‰ä¸ä¼šå¤ªå¤§ã€‚** å¦‚æœæ–‡æœ¬é•¿åº¦ç›¸å·®å¾ˆå¤§ï¼Œå°±åº”è¯¥é‡‡ç”¨ç±»ä¼¼ä¹‹å‰ä»£ç ä¸­çš„åŠ æƒå¹³å‡ï¼Œè¿™æ ·å¯¹äºä¸åŒé•¿åº¦çš„æ–‡æœ¬éƒ½èƒ½è·å¾—è¾ƒä¸ºå‡†ç¡®çš„è¡¨ç¤ºã€‚<br>

## albert_text_vec.pyä¸­ç±»ä¼¼ç”¨æ³•çš„è§£é‡Š:

```python
from transformers import BertTokenizer, AlbertModel
import torch
import numpy as np

class Convert_Text_2_Vector:
    
    tokenizer = BertTokenizer.from_pretrained("clue/albert_chinese_tiny")
    model = AlbertModel.from_pretrained("clue/albert_chinese_tiny")
    
    def __init__(self):
        pass
    def convert_to_vec(self, user_input):
        inputs = self.tokenizer(user_input, return_tensors='pt')
        with torch.no_grad():
                outputs = self.model(**inputs)
        data = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
        data = data / np.linalg.norm(data, axis=0)
        data = [data.tolist()]
        return data
```

æ£€ç´¢çš„æ—¶å€™æ˜¯ä¸€æ¡ä¸€æ¡æ£€ç´¢ï¼Œä¸éœ€è¦padding(paddingæ˜¯å› ä¸ºéœ€è¦batchæ“ä½œ)ã€‚**å¦‚æœæ²¡æœ‰padding(å¡«å……)æ“ä½œï¼Œé‚£æ‰€æœ‰tokenéƒ½æ˜¯æœ‰æ„ä¹‰çš„ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥`mean(dim=1)`**ã€‚<br>

ğŸ’¦ğŸ’¦ğŸ’¦å¦å¤–ï¼Œè§£é‡Šä¸‹ä¸ºä»€ä¹ˆè¾“å…¥æ–‡æœ¬é•¿åº¦ä¸ä¸€ï¼Œä½†ç”Ÿæˆçš„è¯å‘é‡é•¿åº¦ä¾æ—§ä¸º312:<br>

`outputs.last_hidden_state`ç»™å‡ºçš„æ˜¯æ¨¡å‹æ‰€æœ‰éšè—å±‚çš„è¾“å‡ºï¼Œå…¶å½¢çŠ¶é€šå¸¸æ˜¯`[batch_size, sequence_length, hidden_size]`ã€‚å½“ä½ è°ƒç”¨`.mean(dim=1)`æ—¶ï¼Œä½ å®é™…ä¸Šæ˜¯åœ¨æ²¿ç€**åºåˆ—é•¿åº¦ç»´åº¦**å¯¹éšè—çŠ¶æ€è¿›è¡Œå¹³å‡ï¼Œä»è€Œè·å¾—æ¯ä¸ªæ—¶é—´æ­¥ä¸Šç‰¹å¾çš„å¹³å‡å€¼ï¼Œç»“æœçš„å½¢çŠ¶æ˜¯`[batch_size, hidden_size]`ã€‚å› ä¸º`hidden_size`æ˜¯æ¨¡å‹å®šä¹‰çš„å›ºå®šå€¼ï¼Œæ‰€ä»¥å¾—åˆ°çš„å‘é‡é•¿åº¦æ€»æ˜¯312ï¼Œè¿™å¯¹åº”äº`AlbertModel`é…ç½®çš„éšè—å±‚å¤§å°ã€‚<br>