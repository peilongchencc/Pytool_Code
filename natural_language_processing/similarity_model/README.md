# ç›¸ä¼¼åº¦æ¨¡å‹
- [ç›¸ä¼¼åº¦æ¨¡å‹](#ç›¸ä¼¼åº¦æ¨¡å‹)
  - [è¯å‘é‡è½¬åŒ–ï¼š](#è¯å‘é‡è½¬åŒ–)
    - [å•ä¸€æ–‡æœ¬è½¬è¯å‘é‡ï¼š](#å•ä¸€æ–‡æœ¬è½¬è¯å‘é‡)
    - [æ‰¹é‡æ–‡æœ¬è½¬è¯å‘é‡ï¼š](#æ‰¹é‡æ–‡æœ¬è½¬è¯å‘é‡)
    - [å•ä¸€æ–‡æœ¬è½¬è¯å‘é‡å’Œæ‰¹é‡è½¬è¯å‘é‡çš„åŒºåˆ«ï¼š](#å•ä¸€æ–‡æœ¬è½¬è¯å‘é‡å’Œæ‰¹é‡è½¬è¯å‘é‡çš„åŒºåˆ«)
  - [æœ¬åœ°åº”ç”¨å±‚å®ç°ä½™å¼¦ç›¸ä¼¼åº¦çš„è®¡ç®—ï¼š](#æœ¬åœ°åº”ç”¨å±‚å®ç°ä½™å¼¦ç›¸ä¼¼åº¦çš„è®¡ç®—)

## è¯å‘é‡è½¬åŒ–ï¼š

è¿™é‡Œä»¥`albert`æ¨¡å‹ç®€å•ç¤ºèŒƒä¸€ä¸‹å°†æ–‡æœ¬è½¬åŒ–ä¸ºè¯å‘é‡çš„æ–¹æ³•:<br>

### å•ä¸€æ–‡æœ¬è½¬è¯å‘é‡ï¼š

```python
from transformers import BertTokenizer, AlbertModel
import torch
import numpy as np

class Convert_Text_2_Vector:
    
    tokenizer = BertTokenizer.from_pretrained("semantic/clue/albert_chinese_tiny")
    model = AlbertModel.from_pretrained("semantic/clue/albert_chinese_tiny")
    
    def __init__(self):
        pass
    def convert_to_vec(self, user_input):
        inputs = self.tokenizer(user_input, return_tensors='pt')
        with torch.no_grad():
                outputs = self.model(**inputs)
        data = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
        data = data / np.linalg.norm(data, axis=0)
        data = [data.tolist()]  # æ•°æ®æ ¼å¼ä¸º: [[-0.05411861091852188, 0.048285916447639465, -0.029383817687630653, ...]]
        return data
```

### æ‰¹é‡æ–‡æœ¬è½¬è¯å‘é‡ï¼š

```python
class Convert_Batch_Text_2_Vector:
    """å°†æ–‡æœ¬ä»¥batchæ–¹å¼è½¬ä¸ºè¯å‘é‡,æ³¨æ„ä»¥batchæ–¹å‘è½¬è¯å‘é‡éœ€è¦æ¶ˆé™¤paddingçš„å½±å“
    """
    tokenizer = BertTokenizer.from_pretrained("semantic/clue/albert_chinese_tiny")
    model = AlbertModel.from_pretrained("semantic/clue/albert_chinese_tiny")
    
    def __init__(self):
        pass
    def convert_batch_to_embed(self, text_data, batch_size, collection, intentId = None, code = None):
        """å°†æ–‡æœ¬ä»¥batchæ–¹å¼è½¬ä¸ºè¯å‘é‡
        Args:
            text_data: å¾…è½¬åŒ–çš„æ–‡æœ¬,æ•°æ®ç±»å‹è¦æ±‚ä¸ºlist,ä¾‹å¦‚:
                [
                    é—®å¥1,
                    é—®å¥2,
                    è¯è¯­1,
                    è¯è¯­2,
                    ...
                ]
            batch_size: batchå¤§å°
            collection: å‡†å¤‡æ’å…¥çš„milvusé›†åˆ
        Return:
            None: ç”±äºæ‰§è¡Œçš„æ˜¯æ’å…¥æ“ä½œ,æ— è¿”å›å€¼
        """
        # æ£€æŸ¥text_dataæ˜¯å¦æ˜¯å­—ç¬¦ä¸²ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªç›¸åº”çš„åˆ—è¡¨
        if isinstance(text_data, str):
            text_data = [text_data]
        # è·å–å¾…è½¬åŒ–æ–‡æœ¬çš„é•¿åº¦ï¼Œç”¨äºåšbatchåˆ‡åˆ†
        num_texts = len(text_data)
        
        # æ£€æŸ¥intentIdæ˜¯å¦æ˜¯å•ä¸€æ•°å­—ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªç›¸åº”çš„åˆ—è¡¨
        if isinstance(intentId, int):
            intentId = [intentId]
        # å½“è¯è¯­æˆ–é—®å¥æ²¡æœ‰æ„å›¾idæ—¶ï¼Œåˆ›å»ºä¸€ä¸ªä¸æ–‡æœ¬æ•°é‡ç›¸åŒçš„intentIdåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ éƒ½ä¸º`-1`
        elif intentId is None:
            intentId = [-1] * num_texts
        
        if isinstance(code, str):
            code = [code]
        # å½“è¯è¯­æˆ–é—®å¥æ²¡æœ‰wjt_idæ—¶
        elif code is None:
            code = ['-1'] * num_texts
        
        print(f"\nå¼€å§‹è¿›è¡Œè¯å‘é‡æ‰¹é‡è½¬åŒ–ï¼Œå¹¶æŒ‰æ‰¹æ¬¡å†™å…¥Milvusçš„{collection.name}é›†åˆä¸­---\n")
        # è®°å½•è¯å‘é‡è½¬åŒ–å¼€å§‹æ—¶é—´
        start_time = time.time() 
        for i in tqdm(range(0, num_texts, batch_size)):
            
            # å°†æ•°æ®æ‹†åˆ†
            batch_texts = text_data[i:i + batch_size]
            batch_intentId = intentId[i:i + batch_size]
            batch_code = code[i:i + batch_size]
            
            # è¿›è¡Œtokenæ“ä½œ
            encoded_texts = self.tokenizer(batch_texts, return_tensors='pt', padding=True)
            with torch.no_grad():
                # é€šè¿‡æ¨¡å‹è·å–æ–‡æœ¬çš„éšè—çŠ¶æ€
                last_hidden_state = self.model(**encoded_texts).last_hidden_state
                # è·å–æ³¨æ„åŠ›æ©ç 
                attention_mask = encoded_texts["attention_mask"]
                # å°†éšè—çŠ¶æ€ä¸æ³¨æ„åŠ›æ©ç ç›¸ä¹˜ï¼Œç”¨äºæ¶ˆé™¤paddingçš„å½±å“
                last_hidden_state = last_hidden_state * attention_mask.unsqueeze(-1)
                # å¯¹éšè—çŠ¶æ€æ±‚å’Œï¼Œç”¨äºè·å¾—æ•´ä¸ªæ–‡æœ¬çš„è¡¨ç¤º
                sum_hidden_state = last_hidden_state.sum(dim=1).squeeze()
                # é€šè¿‡æ³¨æ„åŠ›æ©ç çš„å’Œè¿›è¡Œå½’ä¸€åŒ–
                output = sum_hidden_state / attention_mask.sum(dim=1, keepdim=True)
                # è½¬åŒ–ä¸ºnumpyæ•°ç»„
                output = output.numpy()
            # å½’ä¸€åŒ–è¾“å‡ºå‘é‡ï¼Œä»¥ä¾¿å‘é‡çš„æ¨¡é•¿ä¸º1
            output = output / np.linalg.norm(output, axis=1, keepdims=True).tolist()
```


### å•ä¸€æ–‡æœ¬è½¬è¯å‘é‡å’Œæ‰¹é‡è½¬è¯å‘é‡çš„åŒºåˆ«ï¼š

åœ¨è¿›è¡Œè¯å‘é‡è½¬æ¢æ—¶ï¼Œæ‰¹é‡å¤„ç†å’Œå•ä¸€æ–‡æœ¬å¤„ç†å­˜åœ¨ä¸€äº›å·®å¼‚ï¼Œä¸»è¦åŸå› åœ¨äºæ•°æ®å¤„ç†å’Œæ¨¡å‹è¾“å…¥çš„éœ€è¦ï¼Œæ‰¹é‡å¤„ç†éœ€è¦è¿›è¡Œpaddingã€‚<br>

1. **æ‰¹é‡å¤„ç†æ—¶ä¸ºä»€ä¹ˆéœ€è¦Padding**:

- **ç»Ÿä¸€é•¿åº¦**ï¼šæ·±åº¦å­¦ä¹ æ¨¡å‹é€šå¸¸éœ€è¦è¾“å…¥æ•°æ®å…·æœ‰ç›¸åŒçš„ç»´åº¦ã€‚åœ¨æ‰¹é‡å¤„ç†æ—¶ï¼Œä¸åŒçš„æ–‡æœ¬å¯èƒ½æœ‰ä¸åŒçš„é•¿åº¦ï¼Œå› æ­¤ä¸ºäº†ä½¿å®ƒä»¬çš„é•¿åº¦ç»Ÿä¸€ï¼Œè¾ƒçŸ­çš„æ–‡æœ¬ä¼šé€šè¿‡paddingï¼ˆå¡«å……ï¼‰æ¥å¢åŠ é•¿åº¦ï¼Œä»¥åŒ¹é…åˆ°æ‰¹é‡ä¸­æœ€é•¿çš„æ–‡æœ¬ã€‚

- **æ•ˆç‡**ï¼šä½¿ç”¨ç›¸åŒé•¿åº¦çš„æ‰¹é‡æ•°æ®å¯ä»¥æé«˜è®¡ç®—æ•ˆç‡ï¼Œå› ä¸ºè¿™å…è®¸æ¨¡å‹ä»¥å›ºå®šçš„å½¢çŠ¶å¤„ç†å¤šä¸ªæ•°æ®æ ·æœ¬ï¼Œä»è€Œæ›´é«˜æ•ˆåœ°ä½¿ç”¨è®¡ç®—èµ„æºã€‚

- **GPUä¼˜åŒ–**ï¼šåœ¨ä½¿ç”¨GPUè¿›è¡Œè®¡ç®—æ—¶ï¼Œå…·æœ‰ç»Ÿä¸€ç»´åº¦çš„æ•°æ®å¯ä»¥æ›´å¥½åœ°å¹¶è¡Œå¤„ç†ï¼Œä»è€Œæé«˜å¤„ç†é€Ÿåº¦ã€‚

2. **å•ä¸€æ–‡æœ¬å¤„ç†æ—¶ä¸ºä»€ä¹ˆä¸éœ€è¦Padding**:

- **å•ä¸ªè¾“å…¥**ï¼šå½“åªå¤„ç†ä¸€ä¸ªæ–‡æœ¬æ—¶ï¼Œä¸å­˜åœ¨é•¿åº¦ä¸ä¸€è‡´çš„é—®é¢˜ã€‚æ¨¡å‹å¯ä»¥ç›´æ¥æ ¹æ®è¯¥æ–‡æœ¬çš„å®é™…é•¿åº¦è¿›è¡Œå¤„ç†ï¼Œæ— éœ€è¿›è¡Œä»»ä½•å¡«å……ã€‚

- **çµæ´»æ€§**ï¼šå•ä¸ªæ–‡æœ¬å¤„ç†æ›´åŠ çµæ´»ï¼Œä¸éœ€è¦è€ƒè™‘ä¸å…¶ä»–æ–‡æœ¬çš„é•¿åº¦åŒ¹é…é—®é¢˜ï¼Œå› æ­¤å¯ä»¥ç›´æ¥æ ¹æ®æ–‡æœ¬æœ¬èº«çš„é•¿åº¦è¿›è¡Œè¯å‘é‡è½¬æ¢ã€‚

- **èµ„æºåˆ©ç”¨**ï¼šåœ¨å•ä¸ªæ–‡æœ¬å¤„ç†ä¸­ï¼Œèµ„æºåˆ©ç”¨ä¸æ˜¯ä¸»è¦è€ƒè™‘å› ç´ ï¼Œå› ä¸ºæ²¡æœ‰å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡æœ¬çš„éœ€æ±‚ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œpaddingä¸»è¦æ˜¯ä¸ºäº†å¤„ç†å¤šä¸ªæ–‡æœ¬æ—¶ï¼Œç¡®ä¿å®ƒä»¬å…·æœ‰ç»Ÿä¸€çš„é•¿åº¦ï¼Œä»è€Œä¾¿äºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„é«˜æ•ˆå¤„ç†ã€‚è€Œåœ¨å•ä¸ªæ–‡æœ¬å¤„ç†æ—¶ï¼Œç”±äºä¸å­˜åœ¨é•¿åº¦ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå› æ­¤ä¸éœ€è¦è¿›è¡Œpaddingã€‚<br>

ğŸš€ğŸš€ğŸš€æ³¨æ„:<br>

è¿™ä¸¤ç§æ–¹å¼å¯¹åŒä¸€ä¸ªæ–‡æœ¬è½¬æ¢æˆçš„è¯å‘é‡æ˜¯ç›¸åŒçš„ï¼Œå¯¹äºåŒä¸€ä¸ªæ–‡æœ¬ï¼Œæ— è®ºæ˜¯å•ç‹¬å¤„ç†è¿˜æ˜¯ä½œä¸ºæ‰¹é‡å¤„ç†çš„ä¸€éƒ¨åˆ†ï¼Œå…¶ç”Ÿæˆçš„è¯å‘é‡é€šå¸¸æ˜¯ç›¸åŒçš„ã€‚<br>

å½“ç„¶ï¼Œå‰ææ˜¯åœ¨ä¸¤ç§æƒ…å†µä¸‹ä½¿ç”¨äº†ç›¸åŒçš„è¯å‘é‡æ¨¡å‹å’Œç›¸åŒçš„å¤„ç†æµç¨‹ã€‚ğŸª´ğŸª´ğŸª´ğŸª´ğŸª´<br>

ğŸš¨ğŸš¨ğŸš¨è¡¥å……:<br>

è¿›è¡Œäº†paddingä¹‹åæ˜¯å¦éœ€è¦æ¶ˆé™¤paddingçš„å½±å“ï¼Œè¿™å–å†³äºä½ ä½¿ç”¨çš„æ¨¡å‹å’Œåº”ç”¨åœºæ™¯ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œpaddingæ˜¯å¿…è¦çš„ï¼Œå¹¶ä¸”å…¶å½±å“æ˜¯è¢«æ¨¡å‹å†…éƒ¨å¤„ç†çš„ï¼›åœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œåˆ™å¯èƒ½éœ€è¦é‡‡å–æªæ–½æ¥å‡å°‘paddingçš„å½±å“ã€‚<br>

## æœ¬åœ°åº”ç”¨å±‚å®ç°ä½™å¼¦ç›¸ä¼¼åº¦çš„è®¡ç®—ï¼š

```python
import sys
import os

# è·å–å½“å‰è„šæœ¬çš„ç»å¯¹è·¯å¾„
current_script_path = os.path.abspath(__file__)
# è·å–å½“å‰è„šæœ¬çš„çˆ¶ç›®å½•çš„çˆ¶ç›®å½•
parent_directory_of_the_parent_directory = os.path.dirname(os.path.dirname(current_script_path))
# å°†è¿™ä¸ªç›®å½•æ·»åŠ åˆ° sys.path
sys.path.append(parent_directory_of_the_parent_directory)

from albert_text_vec import Convert_Text_2_Vector

# è¯å‘é‡è½¬åŒ–ç±»çš„å®ä¾‹åŒ–
embed_model = Convert_Text_2_Vector()

# è·å–ç”¨æˆ·æ•°æ®
usr_text = "å®šæŠ•çš„ä¼˜åŠ¿"
# å°†ç”¨æˆ·æ•°æ®è½¬ä¸ºå‘é‡
usr_text_vector = embed_model.convert_to_vec(usr_text)

# è·å–å¾…æ¯”è¾ƒæ•°æ®
# to_be_compare_text = "å®šæŠ•çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆ"
to_be_compare_text = "å•åªåŸºé‡‘çš„ä¸šç»©èµ°åŠ¿åœ¨å“ªçœ‹"
# å°†å¾…æ¯”è¾ƒæ•°æ®è½¬ä¸ºå‘é‡
to_be_compare_text_vector = embed_model.convert_to_vec(to_be_compare_text)


from sklearn.metrics.pairwise import cosine_similarity

# è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
similarity = cosine_similarity(usr_text_vector, to_be_compare_text_vector)
print("ç›¸ä¼¼åº¦: ", similarity[0][0])
```

å¦‚æœä½ æ²¡æœ‰å®‰è£…`scikit-learn`ï¼Œéœ€è¦è¿è¡Œä¸‹åˆ—æŒ‡ä»¤å…ˆå®‰è£…:<br>

```bash
conda install scikit-learn
```


åœ¨ Milvus ä¸­è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ä¸æˆ‘è¿™é‡Œæä¾›çš„ Python ä»£ç è®¡ç®—æ–¹å¼å¯èƒ½å­˜åœ¨å¾®å°å·®å¼‚ã€‚è¿™ç§å·®å¼‚å¯èƒ½æºäºä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š<br>

1. **æ•°å€¼ç²¾åº¦å’Œæµ®ç‚¹è¿ç®—**ï¼šåœ¨ä¸åŒçš„ç³»ç»Ÿæˆ–åº“ä¸­è¿›è¡Œæµ®ç‚¹æ•°è¿ç®—æ—¶ï¼Œç”±äºåº•å±‚å®ç°å’Œæ•°å€¼ç²¾åº¦çš„ä¸åŒï¼Œå¯èƒ½ä¼šå‡ºç°å¾®å°çš„å·®å¼‚ã€‚Milvus å¯èƒ½åœ¨å†…éƒ¨ä½¿ç”¨ä¸åŒçš„ä¼˜åŒ–å’Œæµ®ç‚¹ç²¾åº¦è®¾ç½®ï¼Œè¿™å¯èƒ½å¯¼è‡´ä¸ `sklearn` åº“çš„ç»“æœç•¥æœ‰ä¸åŒã€‚

2. **ç´¢å¼•å’Œæœç´¢ç­–ç•¥**ï¼šMilvus ä½¿ç”¨ç‰¹å®šçš„ç´¢å¼•ç»“æ„æ¥ä¼˜åŒ–æœç´¢ã€‚å½“ä½ åœ¨ Milvus ä¸­æ„å»ºç´¢å¼•å¹¶æ‰§è¡Œæœç´¢æ—¶ï¼Œå®ƒå¯èƒ½é‡‡ç”¨ä¸€äº›è¿‘ä¼¼ç®—æ³•ä»¥æé«˜æœç´¢æ•ˆç‡ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´ä¸ç›´æ¥è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çš„ç»“æœæœ‰è½»å¾®å·®å¼‚ã€‚

3. **ç‰ˆæœ¬å·®å¼‚**ï¼šä¸åŒç‰ˆæœ¬çš„ Milvus å¯èƒ½åœ¨åº•å±‚å®ç°ä¸Šæœ‰æ‰€ä¸åŒï¼Œè¿™ä¹Ÿå¯èƒ½ä¼šå½±å“è®¡ç®—ç»“æœã€‚

æ€»çš„æ¥è¯´ï¼Œå¦‚æœå·®å¼‚ä¸å¤§ï¼Œè¿™é€šå¸¸æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºåœ¨å®é™…åº”ç”¨ä¸­ï¼Œå®Œå…¨åŒ¹é…çš„ä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†å¹¶ä¸æ€»æ˜¯å¿…éœ€çš„ã€‚å®é™…ä¸Šï¼Œå¾®å°çš„å·®å¼‚é€šå¸¸ä¸ä¼šå¯¹å¤§å¤šæ•°åº”ç”¨äº§ç”Ÿé‡å¤§å½±å“ã€‚å¦‚æœä½ éœ€è¦æ›´ç²¾ç¡®çš„æ§åˆ¶æˆ–ç†è§£è¿™äº›å·®å¼‚ï¼Œå¯èƒ½éœ€è¦æŸ¥çœ‹ Milvus çš„å…·ä½“å®ç°ç»†èŠ‚æˆ–è”ç³»å…¶æŠ€æœ¯æ”¯æŒã€‚<br>